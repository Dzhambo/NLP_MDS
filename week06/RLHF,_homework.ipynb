{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvhPa7a59AIG"
      },
      "source": [
        "<font color=red>**Danger zone:**</font> you'll be fine-tuning a model to generate positive, negative or even toxic reviews. We'll be doing this for fun, but this is also the technique for [review bombing](https://en.wikipedia.org/wiki/Review_bomb), bot farms on social media and other less than dignified stuff. It is ultimately your decision how you apply this knowledge, but before you choose, ask yourself: is this why you chose to learn ML?\n",
        "\n",
        "\n",
        "# LLMs Alignment with Reinforcement Learning from human feedback (RLHF).\n",
        "\n",
        "_based on the [original notebook](https://github.com/antndlcrx/oxford-llms-workshop/blob/main/materials/seminars/day_3/8_LLMs%20alignment%20with%20RLHF.ipynb) by Ilya Boytsov for the Oxford LLMs workshop_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgfL4bSSAXan"
      },
      "source": [
        "In this session, you're gonna fine-tune a language model with reinforcement learning to make it generate good (or bad) reviews.\n",
        "\n",
        "To perform RL-based fine-tuning, we'll use a new (in this course) library called [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl). TRL implements the main reinforcement learning components of RLHF: reward modeling and fine-tuning with PPO.\n",
        "\n",
        "![img](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/TRL-readme.png)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q trl==0.11.0 transformers datasets peft"
      ],
      "metadata": {
        "id": "CGbWuMNjVDWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r2iqDoGaEgHr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cJfrTbFYAx8"
      },
      "source": [
        "### Tutorial: align the model to generate positive movie reviews\n",
        "\n",
        "To see how TRL works, we'll use it to align GPT2 on IMDB dataset to generate positive (or negative) movie reviews. In fact, __it's your choice whether you want positive or negative reviews.__\n",
        "\n",
        "But before you choose, let's take a look at the baseline model: a GPT-2 fine-tuned on generating arbitrary movie reviews."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "main_tokenizer = transformers.AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
        "main_model = transformers.AutoModelForCausalLM.from_pretrained(\"lvwerra/gpt2-imdb\", device_map=device)"
      ],
      "metadata": {
        "id": "pHs22MXdPify"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = main_tokenizer(\"The movie\", return_tensors='pt').to(device)\n",
        "generated_ids = main_model.generate(**inputs, max_new_tokens=50, do_sample=True)\n",
        "print(\"\\nGenerated text:\", main_tokenizer.decode(generated_ids.flatten().cpu().numpy().tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "KE3jo7uhQrvK",
        "outputId": "4ab1f63e-fafc-436e-c95c-ea1ed3236b11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text: The movie, which was recently released in Australia, is actually from the French \"Citizen Kane\". It features four Russian actors and two American actors; one of the actors's Russian girlfriend is in the film. The American actress is in the movie and the Russian\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you run this cell a couple of times, you'll see that the model generates both positive, negative and neutral reviews in some proportion. What we're gonna do next is teach the model to generate more positive (or negative) reviews.\n",
        "\n",
        "Similarly to InstructGPT, we're gonna do that in 2 stages:\n",
        "- **train a reward model** to assign higher values to positive (or negative) reviews\n",
        "- fine-tune the language model to **maximize that reward using [proximal policy optimization](https://openai.com/research/openai-baselines-ppo)**\n",
        "\n"
      ],
      "metadata": {
        "id": "dJbfhMEpR4Sz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcv4uC7xb26Z"
      },
      "source": [
        "## Stage 1: train a reward model\n",
        "\n",
        "First, we'll train a BERT-like model as our reward model. We'll generate a synthetic pairwise rankings to emulate human rankings.\n",
        "\n",
        "__Q:__ why do I need a reward model? Can I just use a pre-trained sentiment classifier? <br> __A:__ Yes, you can - but that only works for movie reviews. But this tutorial will teach you how to do RLHF for any kind objective.\n",
        "\n",
        "\n",
        "__If you actually want to maximize sentiment (or other \"label\") instead of human preferences, train reward model as a classifier! (see week5)__\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll be fine-tuning a small BERT-like model for now. Please try other models for the main assignment.\n",
        "reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", device_map=device)\n",
        "reward_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
      ],
      "metadata": {
        "id": "WeOdZ_ayc9dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Note that__ the reward model has a separate tokenizer, different from the main model. They don't need to be the same for RLHF fine-tuning."
      ],
      "metadata": {
        "id": "_ZUUNQo-d11b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To train a reward model, you need a dataset (or generator) of positive-negative pairs.\n",
        "# Each training sample should be a dict with 4 keys:\n",
        "#  - input_ids_chosen, attention_mask_chosen = tokenizer(\"A sentence that human labeler likes more\")\n",
        "#  - input_ids_rejected, attention_mask_rejected = tokenizer(\"A sentence that human labeler likes less\")\n",
        "\n",
        "import torch\n",
        "import datasets\n",
        "\n",
        "class IMDBPairwiseDataset(torch.utils.data.Dataset):\n",
        "    \"\"\" A dataset of all possible pairs of chosen and texts in TRT reward training format \"\"\"\n",
        "    def __init__(self, imdb, tokenizer, accepted_label: int):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.chosen_texts = [row['text'] for row in imdb if row['label'] == accepted_label]\n",
        "        self.rejected_texts = [row['text'] for row in imdb if row['label'] != accepted_label]\n",
        "        self.column_names = ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected']\n",
        "        assert self.chosen_texts, f\"no texts with label {accepted_label}\"\n",
        "        print(f\"Found {len(self.chosen_texts)} chosen and {len(self.rejected_texts)} rejected texts, {len(self)} pairs\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.chosen_texts) * len(self.rejected_texts)  # all pairs\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        chosen = self.tokenizer(self.chosen_texts[index // len(self.chosen_texts)], truncation=True)\n",
        "        rejected = self.tokenizer(self.rejected_texts[index % len(self.chosen_texts)], truncation=True)\n",
        "        return dict(input_ids_chosen=chosen['input_ids'], attention_mask_chosen=chosen['attention_mask'],\n",
        "                    input_ids_rejected=rejected['input_ids'], attention_mask_rejected=rejected['attention_mask'])"
      ],
      "metadata": {
        "id": "TTWR-48ZXQX6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_LABEL = 0  # and make sure it works by reviewing the sample printed below\n",
        "imdb = datasets.load_dataset(\"imdb\", split='train')\n",
        "reward_data = IMDBPairwiseDataset(imdb, reward_tokenizer, accepted_label=TARGET_LABEL)\n",
        "\n",
        "sample = reward_data[31337]\n",
        "print('CHOSEN:', reward_tokenizer.decode(sample['input_ids_chosen']))\n",
        "print('REJECTED:', reward_tokenizer.decode(sample['input_ids_rejected']))"
      ],
      "metadata": {
        "id": "olo-bvgNcwEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using `trl.RewardTrainer` - a special case of `transformers.Trainer` that you used in the past. `RewardTrainer` accepts the same format of training arguments (e.g. batch size, gradient checkpointing) as before, except that it trains the model for the pairwise reward objective from [the InstructGPT paper](https://arxiv.org/pdf/2203.02155.pdf):\n",
        "\n",
        "![img](https://i.imgur.com/2JzNAPs.png)\n",
        "\n",
        "Note that the model itself does not score pairs: it processes chosen ($y_w$) and rejected ($y_l$) samples independently. To minimize this loss, the reward model needs to score chosen sample higher than the rejected one. Note that the formula also assumes some context $x$, which is useful for seq2seq tasks. In our case of movie reviews, $x$ is empty."
      ],
      "metadata": {
        "id": "kZRczyofiSl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import trl\n",
        "\n",
        "training_args = trl.RewardConfig(  # like transformers.TrainingArguments\n",
        "    output_dir=\"reward_model\",\n",
        "    per_device_train_batch_size=32,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1.41e-5,\n",
        "    max_steps=1_000,              # note: training may need more than 1k steps\n",
        "    logging_steps=50,\n",
        "    gradient_checkpointing=True,  # reduce memory usage but train ~30% slower\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    fp16=True                     # disable this on CPU or on very old GPUs\n",
        "    # you may add any other hyperparameters that you found useful in weeks 5-7\n",
        ")\n",
        "\n",
        "trainer = trl.RewardTrainer(\n",
        "    model=reward_model,\n",
        "    args=training_args,\n",
        "    tokenizer=reward_tokenizer,\n",
        "    train_dataset=reward_data,\n",
        "    peft_config=None,  # optionally, you may tune with LoRA, prompt-tuning, etc\n",
        ")\n",
        "\n",
        "\n",
        "#trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaQ_-JAzakJs",
        "outputId": "c8c835ce-de79-49ed-b9eb-04dcf2853820"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/reward_trainer.py:182: UserWarning: When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig. It will be set to `512` by default, but you should do it yourself in the future.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/reward_trainer.py:199: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/reward_trainer.py:208: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `RewardTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reward_model.gradient_checkpointing_disable()\n",
        "reward_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRk7z-2r4C-A",
        "outputId": "3a9dd978-61e1-4dbf-9cb9-943115341248"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sanity-check the reward model (1 point)\n",
        "\n",
        "Let's check how our reward model performs.\n",
        "\n",
        "__Your task__ is to measure how often does your reward model can rank a pair of (chosen and rejected) reviews correctly. Please measure this separately for train data (`imdb`) and a separate test set loaded below."
      ],
      "metadata": {
        "id": "wZIaS-gRo8yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for sample_index in 45, 16000:\n",
        "  print('TEXT:', imdb[sample_index]['text'])\n",
        "  inputs = reward_tokenizer(\n",
        "      imdb[sample_index]['text'], truncation=True, return_tensors='pt').to(device)\n",
        "  with torch.no_grad():\n",
        "    reward = reward_model(**inputs).logits[0, 0].item()\n",
        "    print(\"REWARD:\", reward)\n",
        "  print('LABEL:', imdb[sample_index]['label'])\n",
        "  print()\n",
        "\n",
        "# note: your reward model may produce different absolute rewards.\n",
        "# This is fine as long as the rewards are ordered correctly (most of the time)"
      ],
      "metadata": {
        "id": "IeQ108nOZ7nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_test = datasets.load_dataset(\"imdb\", split='test')\n",
        "\n",
        "# <a whole lot of your code here, feel free to spit it as you see fit>"
      ],
      "metadata": {
        "id": "aEevUrfqavnb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reward-guided generation (1 point)\n",
        "\n",
        "If you did everything right, by now you should have a decent reward model. Before we use it for reinforcement learning, let's see if we can align model samples without any training.\n",
        "\n",
        "To do so, you can use reward-guided inference: __generate N=16 samples, then select the one with the highest reward__ (according to your reward model).\n",
        "\n",
        "For this problem, it's on you to demonstrate whether or not your code works. Find at least 5 neutral prompts such as \"This movie is\" (...), generate samples, rank them based on reward and show which samples get the highest reward.\n",
        "\n",
        "Note: it is faster to generate samples in parallel, rather than sequentially, as follows:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NHCWHMyRw2-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = main_tokenizer([\"It was\"] * 5, return_tensors='pt').to(device)\n",
        "for candidate in main_model.generate(**inputs, max_new_tokens=50, do_sample=True):\n",
        "  print(\"Sample:\", main_tokenizer.decode(candidate.flatten().cpu().numpy().tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BRsyb2cq5dR",
        "outputId": "ef48f75f-f8b8-46f8-d273-9b8a7667fb5a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample: It was my only time in the hospital where my family was not in any shape to see any of this. The story was so poorly told that I couldn't feel any emotions with the actors. I did go to an Indian church when it was opened but the\n",
            "Sample: It was obvious, though, that the majority of the time it would be a good idea to go to sleep. The fact that my \"girlfriend\" doesn't sleep, or even has nightmares, was the first sign of trouble for me. I would just let\n",
            "Sample: It was quite amazing! Very realistic, very realistic! The movie is very realistic! The acting is absolutely unbelievable! I can not believe this movie made it in the USA. I wish it weren't made in Denmark! If it wasn't for Denmark then this\n",
            "Sample: It was not one of my favorite movies of the year. However, I really enjoyed it--it was so original and so unique. I love great films and this really was. I would be surprised if anyone else has seen it.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Sample: It was almost like the filmmakers had a bad taste. The actors were all terrible, and were even worse than most of the rest, though to put it bluntly, the characters were almost too stupid to exist.<br /><br />The entire movie seemed like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <YOUR CODE HERE> - feel free to organize it as you see fit"
      ],
      "metadata": {
        "id": "r08F4lz7yxE1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NjQ40BRoH5f"
      },
      "source": [
        "# Stage 2: fine-tune the main model with RL\n",
        "\n",
        "\n",
        "For this tutorial, we will optimize GPT2 to produce positive IMDB movie reviews using the reward model you trained above.\n",
        "\n",
        "Unlike supervised fine-tuning, RL allows model to generate it's own sentences on each training step. Then, it calculates the reward of those specific sentences, and finally, updates the model to increase the probability of sentences with high reward.\n",
        "\n",
        "Thus, each RLHF consists of three stages: __Rollout__, __Evaluation__ and __Update__\n",
        "\n",
        "<div style=\"text-align: center\">\n",
        "<img src='https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/gpt2_bert_training.png' width='600'>\n",
        "\n",
        "The update stage depends on the specific RL algorithm. We'll be using Proximal Policy Optimization, or [PPO](https://arxiv.org/abs/1707.06347), similarly to what was used for InstructGPT.\n",
        "\n",
        "Before we run those 3 stages, however, we need to create a dataset of \"queries\" - partial reviews in our case."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl.core import LengthSampler"
      ],
      "metadata": {
        "id": "I40Q-zz3HQJ9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: this code is specific to IMDB; you will need to re-write it for other tasks\n",
        "imdb_for_rlhf = imdb.filter(lambda row: len(row['text']) > 200, batched=False)\n",
        "imdb_for_rlhf = imdb_for_rlhf.remove_columns(['label'])\n",
        "sample_length = LengthSampler(2, 8)  # use the first 2-8 tokens as query\n",
        "\n",
        "def select_query_and_tokenize(sample):\n",
        "    query_ids = main_tokenizer.encode(sample[\"text\"])[: sample_length()]\n",
        "    sample[\"query\"] = main_tokenizer.decode(query_ids)  # query is the only required column\n",
        "    sample[\"input_ids\"] = query_ids  # to avoid re-tokenizing later\n",
        "    return sample  # we do not need the rest - it will be generated by the model\n",
        "\n",
        "imdb_for_rlhf = imdb_for_rlhf.map(select_query_and_tokenize, batched=False)\n",
        "imdb_for_rlhf.set_format(type=\"torch\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119,
          "referenced_widgets": [
            "06b42644de3349fd81b1c269207353b6",
            "fafc4701204b49a88405d2ecc916e073",
            "e1afc9726f69407082f26d510aa73cc3",
            "c63bdfa28ed34f0eb7e07c28aa0fde36",
            "e3a72801ee754f39b99f545e5d94d8b6",
            "56f1f7d5a3e44039ab184c30faaa2e69",
            "666ca83d33734a1aa437f895cb328fde",
            "eb524b08c67b4f36942f5993a49ab911",
            "e81bb993c204493a88b963b0dfa17bf7",
            "74f7bc1dab89426a800d41cfe97c7d03",
            "fbcd42a3d79145219311ccfc3406b631",
            "df7afbf7934642e7abfc0f4a3f70a121",
            "4bfc25e7972d441bbe93b85bf92dfc84",
            "20930b5ff6524850b38aad60f6fb8708",
            "0787f76c202d467990c15fa53524daa3",
            "ed3743bc60414ae3b6d51e53fd3c4286",
            "084ad4814f2045eeb0ba5f1668aee016",
            "de77d51014dc45a2b4bac2f6eef47df8",
            "c1fa16c0d7c44497bb98236ba458374a",
            "3682f5394215443883524fcf31cce3cc",
            "8cfe31c642ac43618da696e966eefa87",
            "a6c59cae4e3a4550917aca4e1183b438"
          ]
        },
        "id": "jm5IUrer0xd_",
        "outputId": "44d46c5e-50ee-49a2-e8c7-09f7fc76fde4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06b42644de3349fd81b1c269207353b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/24895 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df7afbf7934642e7abfc0f4a3f70a121"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's prepare your reward model to predict rewards on whatever reviews were generated. Note that we use plaintext reviews because main model uses a different tokenizer from the reward model."
      ],
      "metadata": {
        "id": "lKIAyilP3Bf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "def compute_reward(texts: List[str]) -> torch.Tensor:\n",
        "  inputs = reward_tokenizer(texts, truncation=True, padding=True, return_tensors='pt').to(device)\n",
        "  with torch.no_grad():\n",
        "    return reward_model(**inputs).logits[:, 0]"
      ],
      "metadata": {
        "id": "kkm4MLOr20Jk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_reward([imdb[45]['text'], imdb[16000]['text']])  # test on human-written reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wJto13M3vWu",
        "outputId": "c2fc8ae8-838d-4f1a-9633-437a2a07e569"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.1517, -0.1592])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we move to RL training. In this tutorial, we'll train LoRA adapters and not the full model."
      ],
      "metadata": {
        "id": "U3buACYV4QLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import peft\n",
        "peft_config = peft.LoraConfig(\n",
        "    task_type=peft.TaskType.CAUSAL_LM, r=32, lora_alpha=32, lora_dropout=0.0, inference_mode=False\n",
        ")\n",
        "\n",
        "# reload main model as AutoModelForCausalLMWithValueHead - with an extra head needed for PPO\n",
        "main_tokenizer = transformers.AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
        "main_tokenizer.pad_token = main_tokenizer.eos_token\n",
        "\n",
        "main_model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(\"lvwerra/gpt2-imdb\", device_map=device)\n",
        "main_model = peft.get_peft_model(main_model, peft_config, adapter_name='default')\n",
        "main_model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nar1yXgl4KQa",
        "outputId": "35ad4b29-71bc-4044-bf4f-664a3fbd61d9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,179,648 || all params: 125,620,225 || trainable%: 0.9391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as before, trl has a special type of trainer that minimize PPO-specific pseudo-loss. You can read more on this trainer [here](https://huggingface.co/docs/trl/main/en/ppo_trainer)."
      ],
      "metadata": {
        "id": "qIQK5bcpCPZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = trl.PPOConfig(\n",
        "    model_name=main_model.config._name_or_path,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1.41e-5,\n",
        "    batch_size=64,\n",
        "    mini_batch_size=4,\n",
        "    ppo_epochs=4,                 # PPO performs this many updates per training batch\n",
        ")\n",
        "\n",
        "ppo_trainer = trl.PPOTrainer(\n",
        "    training_args, model=main_model.model, tokenizer=main_tokenizer,\n",
        "    dataset=imdb_for_rlhf, data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0])\n",
        ")  # note: we pass main_model.model because PPOTrainer checks for one of several supported model types ...\n",
        "# ... main_model.model is a model with adapters, which is supported. main_model itself is a wrapper that is not supported"
      ],
      "metadata": {
        "id": "EvTtiLs94txE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e0aafc-bdcc-4958-89bc-99dc17e787e2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "max_steps = 50   # can be insufficient for some tasks - watch your learning curves\n",
        "generation_kwargs = dict(\n",
        "    min_length=-1, max_new_tokens=128, do_sample=True, top_k=0, top_p=1.0, pad_token_id=main_tokenizer.eos_token_id)\n",
        "#                                  ^-- task-specific parameter!\n",
        "with tqdm(enumerate(ppo_trainer.dataloader), total=max_steps) as progressbar:\n",
        "  # note: ppo_trainer.dataloader is just a regular dataloader of queries, no RL-specific magic :)\n",
        "  for epoch, batch in progressbar:\n",
        "    if epoch >= max_steps:\n",
        "        break\n",
        "\n",
        "    # Rollout stage: generate continuations from batch queries using main_model\n",
        "    response_tensors = ppo_trainer.generate(batch['input_ids'], **generation_kwargs)\n",
        "    # ^-- list of tensors of token ids from main model tokenizer\n",
        "\n",
        "    # de-tokenize responses to strings (since reward model uses a different tokenizer)\n",
        "    batch[\"response\"] = [main_tokenizer.decode(response.squeeze()) for response in response_tensors]\n",
        "    # note: response_tensors already contain query tokens, so we don't need to add queries manually.\n",
        "    # This may not be true for other tasks: check this manually by viewing batch[\"response\"] and batch[\"query\"]\n",
        "\n",
        "\n",
        "    # Evaluation stage\n",
        "    rewards = compute_reward(batch['response'])\n",
        "\n",
        "    # Update stage\n",
        "    stats = ppo_trainer.step(batch['input_ids'], response_tensors, list(rewards.split(1)))\n",
        "    stats['rewards/mean'] = rewards.mean().item()\n",
        "\n",
        "    print(\"-\" * 30, 'STEP', epoch, '-' * 30)\n",
        "    print(f'rewards/mean:\\t{stats[\"rewards/mean\"]:.9f}\\t<---- average reward over this batch (higher=better, noisy)')\n",
        "    print(f'ppo/returns/mean:\\t{stats[\"ppo/returns/mean\"]:.9f}\\t<---- model-estimated average discounted reward')\n",
        "    print(f'objective/kl:\\t{stats[\"objective/kl\"]:.9f}\\t<---- how far we are from the original model (regularizer)')\n",
        "    print()\n",
        "\n",
        "    ppo_trainer.log_stats(stats, batch, list(rewards.split(1)))"
      ],
      "metadata": {
        "id": "eYr-w666-QfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main assignment - <u>actually</u> train the model (8 points)\n",
        "\n",
        "\n",
        "Your main task for this week is to use the RLHF pipeline to train a model for a reward of your choice. Here's what you can choose from:\n",
        "\n",
        "__A. Toxicity fine-tuning:__ train the model to be less (or more!) toxic. For this task, you may use the data from [jigsaw toxic comments](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) and [lmsys/toxic-chat](https://huggingface.co/datasets/lmsys/toxic-chat),  or any other source. Alternatively, you may use toxicity scores from [oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1).\n",
        "\n",
        "\n",
        "#### General tips & tricks\n",
        "\n",
        "\n",
        "Things to look out for:\n",
        "- during PPO stage, the reward model should be in eval mode (dropout disabled)\n",
        "- make sure max_length and max_new_tokens are enough for your chosen dataset - at least most of the time\n",
        "- when in doubt, view the data manually or inspect how the model performs on a few samples\n",
        "\n",
        "\n",
        "We highly recommend that you manually check the performance after each sub-stage:\n",
        "1. when you assembled the pairwise dataset, inspect a couple of from of *your* dataset class and detokenize them. Make sure that you-the-human understand why one sample was accepted and the other - rejected. At least most of the time. This also lets you spot tokenization/truncation errors.\n",
        "2. after you trained a reward model, measure how accurate this model is in isolation. If your reward model is poor, any subsequent RLHF will also fail.\n",
        "3. once you've trained the main model with RL, ask it to generate examples and explore how well it does. If it produces an obviously bad output, check if the reward model assigns high reward to that output. If yes, reward model is the culprit; if no, it's a question of better/longer PPO training.\n",
        "\n",
        "__It is also a good idea to periodically print samples during training.__\n",
        "\n",
        "__When stuck, simplify the problem.__ If you've spent a several hours enchanting the reward model but it still won't budge, try switching to a simple subtask. For instance, if you're training on hh-rlhf, try limiting it the dataset to 10% of the shortest sequences - they are typically easier to learn.\n",
        "\n",
        "\n",
        "## Assignment stages (and grading)\n",
        "\n",
        "Regardless of the specific task you chose, your solution needs to contain several parts that will be graded separately.\n",
        "\n",
        "\n",
        "#### Stage 1: reward model (4 points)\n",
        "\n",
        "Construct a dataset for training the reward model on your problem. Then, train a reward model on that dataset and evaluate how well can your model predict preferences on a hold-out (test) subset of your data.\n",
        "\n",
        "Please make sure that the part of your notebook where you evaluate reward model is clearly visible and reasonably easy to read. And for all that is holy, do not call it IMDB unless it actually **is** data of imdb movie reviews :)\n",
        "\n",
        "__Not all tasks require a reward model for later PPO fine-tuning.__ For instance, there's no reason to train a reward model if your reward equals sentence length. Likewise, toxicity reward can be estimated with a pre-trained toxicity classifier. __If your task does not require training a reward model, please train an unrelated model on [hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf) as though you were solving assignment version B.__ This is for grading purposes only, you won't use this model for stage 2.\n",
        "\n",
        "\n",
        "#### Stage 2: RL fine-tuning (4 points)\n",
        "\n",
        "Once the reward model is ready - or you can compute rewards without a model - it is time to maximize that reward with PPO. Optionally, you may replace PPO with another RL algorithm (or unlikelihood learning scheme), but only if you're feeling adventurous.\n",
        "\n",
        "\n",
        "First, you need to choose a language model to be fine-tuned. You may choose any model, but make sure that your model **can** generate the data in your format. For instance, [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) is a general purpose LM and may (or may not) need prompt engineering to generate chat assistant responses. For that reason, it is best if you **do not use `\"lvwerra/gpt2-imdb\"` unless you're generating only movie reviews**.\n",
        "\n",
        "\n",
        "\n",
        "There are two \"difficulty modes\" for this task:\n",
        "For the **easy mode**, use [gpt2-large](https://huggingface.co/gpt2-large) or [opt-1.3b](https://huggingface.co/facebook/opt-1.3b) with minimal code changes.\n",
        "If you want the **Hard mode:** use a larger (e.g. 7B) model in combination with `load_in_4bit` and LoRA, the same way we did last week.\n",
        "Some reasonable model choices are [LLaMA-7B](https://huggingface.co/Enoch/llama-7b-hf), [Falcon-7b](https://huggingface.co/tiiuae/falcon-7b), [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) for general-purpose LM or [guanaco-7b](https://huggingface.co/timdettmers/guanaco-7b), [vicuna-7b](https://huggingface.co/lmsys/vicuna-7b-v1.5) for chat-based tasks, though there are many more (see [leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)). In the hard mode, you will need to modify the training arguments to enable 4-bit fine-tuning. Furthermore, your experiments will take somewhat longer to complete. On the plus side, your model will produce significantly better results.\n",
        "\n",
        "__High reward is not enough!__ RL algorithms are famous for [cheating their reward functions](https://openai.com/research/faulty-reward-functions). To ensure that your model is actually doing what you want it to do, you will need some additional evaluation. To get the full grade, provide at least 20 side-by-side examples of your fine-tuned model vs original model predictions and a short summary.\n",
        "\n",
        "Alternatively, you may provide 5 examples and some extrinsic evaluation metric over many examples. For instance, you may use a different pre-trained toxicity score for option A. When dealing with human preferences, you may choose to [enlist actual humans](https://toloka.ai/) or [ask GPT4/Claude](https://arxiv.org/pdf/2304.03277.pdf) to compare your model's predictions. For task C, when optimizing for simple rewards like sentence lengths, it is enough to compare histograms of rewards (e.g. average lengths).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hgtmjtilq6T8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L0PgqPPGIMP0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12 (main, Mar 26 2022, 15:51:15) \n[Clang 13.1.6 (clang-1316.0.21.2)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "4c8ff454cd947027f86954d72bf940c689a97dcc494eb53cfe4813862c6065fe"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06b42644de3349fd81b1c269207353b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fafc4701204b49a88405d2ecc916e073",
              "IPY_MODEL_e1afc9726f69407082f26d510aa73cc3",
              "IPY_MODEL_c63bdfa28ed34f0eb7e07c28aa0fde36"
            ],
            "layout": "IPY_MODEL_e3a72801ee754f39b99f545e5d94d8b6"
          }
        },
        "fafc4701204b49a88405d2ecc916e073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56f1f7d5a3e44039ab184c30faaa2e69",
            "placeholder": "​",
            "style": "IPY_MODEL_666ca83d33734a1aa437f895cb328fde",
            "value": "Filter: 100%"
          }
        },
        "e1afc9726f69407082f26d510aa73cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb524b08c67b4f36942f5993a49ab911",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e81bb993c204493a88b963b0dfa17bf7",
            "value": 25000
          }
        },
        "c63bdfa28ed34f0eb7e07c28aa0fde36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74f7bc1dab89426a800d41cfe97c7d03",
            "placeholder": "​",
            "style": "IPY_MODEL_fbcd42a3d79145219311ccfc3406b631",
            "value": " 25000/25000 [00:00&lt;00:00, 104658.35 examples/s]"
          }
        },
        "e3a72801ee754f39b99f545e5d94d8b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56f1f7d5a3e44039ab184c30faaa2e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "666ca83d33734a1aa437f895cb328fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb524b08c67b4f36942f5993a49ab911": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e81bb993c204493a88b963b0dfa17bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74f7bc1dab89426a800d41cfe97c7d03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbcd42a3d79145219311ccfc3406b631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df7afbf7934642e7abfc0f4a3f70a121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4bfc25e7972d441bbe93b85bf92dfc84",
              "IPY_MODEL_20930b5ff6524850b38aad60f6fb8708",
              "IPY_MODEL_0787f76c202d467990c15fa53524daa3"
            ],
            "layout": "IPY_MODEL_ed3743bc60414ae3b6d51e53fd3c4286"
          }
        },
        "4bfc25e7972d441bbe93b85bf92dfc84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_084ad4814f2045eeb0ba5f1668aee016",
            "placeholder": "​",
            "style": "IPY_MODEL_de77d51014dc45a2b4bac2f6eef47df8",
            "value": "Map: 100%"
          }
        },
        "20930b5ff6524850b38aad60f6fb8708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1fa16c0d7c44497bb98236ba458374a",
            "max": 24895,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3682f5394215443883524fcf31cce3cc",
            "value": 24895
          }
        },
        "0787f76c202d467990c15fa53524daa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cfe31c642ac43618da696e966eefa87",
            "placeholder": "​",
            "style": "IPY_MODEL_a6c59cae4e3a4550917aca4e1183b438",
            "value": " 24895/24895 [00:43&lt;00:00, 635.29 examples/s]"
          }
        },
        "ed3743bc60414ae3b6d51e53fd3c4286": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "084ad4814f2045eeb0ba5f1668aee016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de77d51014dc45a2b4bac2f6eef47df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1fa16c0d7c44497bb98236ba458374a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3682f5394215443883524fcf31cce3cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cfe31c642ac43618da696e966eefa87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6c59cae4e3a4550917aca4e1183b438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}